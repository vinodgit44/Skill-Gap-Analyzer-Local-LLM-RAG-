



# ğŸ“„ **README.md â€” JDâ€“Resume Skill Gap Analyzer (Local LLM + RAG)**

<p align="center">
  <img src="assets/banner.png" alt="JDâ€“Resume Analyzer Banner" width="100%">
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10%2B-blue" />
  <img src="https://img.shields.io/badge/LangChain-1.x-orange" />
  <img src="https://img.shields.io/badge/ChromaDB-Local%20Vector%20DB-green" />
  <img src="https://img.shields.io/badge/LLM-TinyLlama%20%2F%20Qwen%20%2F%20GPT2-red" />
  <img src="https://img.shields.io/badge/UI-Streamlit-ff69b4" />
  <img src="https://img.shields.io/badge/License-MIT-yellow" />
</p>

---

# ğŸš€ **JDâ€“Resume Skill Gap Analyzer (Local LLM + RAG)**

A privacy-preserving **AI career analysis tool** that compares your **resume** with any **job description** using a **fully local RAG pipeline** powered by:

* **LangChain v1.x**
* **ChromaDB**
* **MiniLM Embeddings**
* **Local LLMs (TinyLlama / GPT-2 / Qwen)**
* **Streamlit UI**

> âš¡ Works with **Local CPU**, **Local GPU (future)**, and **Kaggle Free GPUs**
> ğŸ”’ 100% private â€” no external API calls required
> ğŸ§  Optional Cloud LLM support (OpenAI, Gemini, Groq, DeepSeek)

---

# ğŸ§  **Features**

### ğŸ” Automatic Skill Extraction

* Extracts technical, domain, and soft skills from both **resume** and **JD**.

### ğŸ“Š Skill Gap Analysis

* Identifies matching, partial, and missing skills.

### ğŸ¯ Learning Roadmap

* Creates a custom **study plan** + **5 real-world AI/ML project ideas**.

### ğŸ§® Job-Fit Score

* Predicts how well the resume matches the JD â†’ score out of 100.

### ğŸ’» Multi-Mode Support

* Local CPU mode
* Local GPU mode (future RTX GPUs)
* Kaggle T4 free GPU mode
* Cloud API mode (GPT-4 / Gemini / Groq / DeepSeek)

### ğŸ›¡ï¸ Privacy

* Everything runs locally â†’ safe for resumes and sensitive data.

---

# ğŸ—ï¸ **Tech Stack**

| Component             | Technology                     |
| --------------------- | ------------------------------ |
| Embeddings            | MiniLM (SentenceTransformers)  |
| Vector DB             | ChromaDB                       |
| Framework             | LangChain v1.x (manual RAG)    |
| UI                    | Streamlit                      |
| Local LLMs            | TinyLlama / GPT-2 / Qwen       |
| Cloud LLMs (Optional) | OpenAI, Gemini, Groq, DeepSeek |
| Hardware              | CPU / GPU Auto-detect          |
| Cloud                 | Kaggle Free GPU                |

---

# ğŸ“ **Project Structure**

```
job-analyzer-basic/
â”‚
â”œâ”€â”€ app.py                 # CLI version (CPU/GPU auto)
â”œâ”€â”€ streamlit_app.py       # Streamlit UI version
â”‚
â”œâ”€â”€ sample_data/
â”‚   â”œâ”€â”€ sample_resume.txt
â”‚   â””â”€â”€ sample_jd.txt
â”‚
â”œâ”€â”€ chroma_store/          # Auto-generated vector DB
â”œâ”€â”€ tmp/                   # Uploaded resume/JD files
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# âš™ï¸ **Installation**

## 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/<your-username>/job-analyzer-basic.git
cd job-analyzer-basic
```

## 2ï¸âƒ£ Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate
```

## 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

# ğŸƒ **How to Run**

## â–¶ï¸ Run CLI Version

```bash
python app.py
```

---

## ğŸŒ Run Streamlit Web UI

```bash
streamlit run streamlit_app.py
```

Then open:

ğŸ‘‰ [http://localhost:8501/](http://localhost:8501/)

Upload **resume.txt** and **jd.txt** â†’ click **Run Analysis**.

---

# âš¡ **Execution Modes (CPU, GPU, Kaggle, API)**

Your code auto-detects GPU:

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
```

## ğŸŸ© 1. Local CPU Mode (Default)

âœ” Free
âœ” Offline
âœ” Works everywhere
âœ” Safe for confidential resumes

âŒ Slower
âŒ Small models only (TinyLlama, GPT-2)

**Recommended models:**

* TinyLlama 1.1B
* GPT-2 / DistilGPT-2
* MiniLM embeddings

---

## ğŸŸ¦ 2. Local GPU Mode (Future GPUs â€” RTX cards)

(*Your GTX 1060 is too old; but this is ready for future upgrades.*)

âœ” Fast inference
âœ” Can run 3Bâ€“14B models
âœ” Best accuracy

âŒ Requires modern NVIDIA GPU

**Recommended:**

* Qwen 1.5Bâ€“4B
* Gemma 2B
* Llama 3B / 8B

---

## ğŸŸª 3. Kaggle Free GPU Mode

Use free **Tesla T4 GPU (16GB)**.

âœ” Free
âœ” Runs 2Bâ€“8B models
âœ” Zero setup

âŒ Timeout after inactivity

**Recommended:**

* Qwen 2.5B / 4B
* Gemma 2B
* Llama 3Bâ€“8B

---

## ğŸ”‘ 4. Cloud API Mode (OpenAI, Gemini, Groq, DeepSeek)

### âœ” Pros:

* Best accuracy
* Fastest processing
* No hardware needed
* Handles long resumes & large JDs

### âŒ Cons:

* Paid
* Internet required
* Privacy concerns

**Recommended Models:**

* GPT-4.1
* GPT-4o-mini
* Gemini 1.5 Pro
* Groq Llama-3-8B
* DeepSeek Chat

---

# âš”ï¸ **API Key vs No API Key â€” Side-by-Side Comparison**

| Feature        | Local (No API Key) | Cloud (API Key)    |
| -------------- | ------------------ | ------------------ |
| Cost           | Free               | Paid ($)           |
| Speed          | Medium             | Very fast          |
| Accuracy       | Medium             | Highest            |
| Privacy        | 100% Local         | Data sent to cloud |
| Hardware Needs | CPU/GPU            | None               |
| Resume Safety  | Excellent          | Medium             |
| Model Size     | â‰¤1.5B              | â‰¤100B+             |
| Setup          | Medium             | Easy               |

---

# ğŸ—ï¸ **Architecture**

## ASCII Architecture Diagram

```
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚     Input Layer         â”‚
                      â”‚  Resume.txt + JD.txt    â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Document Loaders       â”‚
                     â”‚  (LangChain Community)   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚      Embeddings          â”‚
                     â”‚  MiniLM-L6 (CPU/GPU)     â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚      ChromaDB Vector      â”‚
                     â”‚          Store            â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                         (Top-k relevant chunks)
                                   â”‚
                                   â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚        RAG Block         â”‚
                     â”‚ Prompt + Retrieved Docs  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚     Local LLM Engine      â”‚
                     â”‚ TinyLlama / GPT2 / Qwen   â”‚
                     â”‚ (CPU/GPU Auto-Detect)     â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚      Output Layer         â”‚
                     â”‚ Skills â€¢ Gaps â€¢ Roadmap   â”‚
                     â”‚ Job-Fit Score â€¢ Insights  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Mermaid Diagram (GitHub Supported)

```mermaid
flowchart TD
    A[Resume.txt + JD.txt] --> B[Document Loaders<br>LangChain Community]
    B --> C[Embeddings<br>MiniLM-L6 (CPU/GPU)]
    C --> D[ChromaDB<br>Vector Store]
    D --> E[RAG Pipeline<br>Prompt + Retrieved Docs]
    E --> F[Local LLM<br>TinyLlama / GPT-2 / Qwen]
    F --> G[Results<br>Skills â€¢ Gaps â€¢ Roadmap â€¢ Score]
```

---

# ğŸ§ª **Sample Output**

```
ğŸ“Œ JD Skills:
- Python, SQL, NLP, Transformers, AWS

ğŸ“Œ Resume Skills:
- Python, NLP, TensorFlow, Docker

ğŸ“Š Skill Gap:
Missing â†’ AWS, CI/CD, Airflow  
Partial â†’ ML Ops  

ğŸ¯ Learning Roadmap:
1. AWS basics â†’ ECS/Lambda project  
2. Build CI/CD pipeline  
3. Airflow ETL pipeline  

ğŸ§® Job Fit Score: 78/100
```

---

# ğŸ”§ **Troubleshooting**

### CUDA error?

Your GTX 1060 is too old â†’ CPU fallback is automatic.

### Slow generation?

Use Kaggle free GPU (T4).

### Want more accuracy?

Use API Key mode.

---

# ğŸ› ï¸ **Future Enhancements**

* PDF upload support
* Report export (PDF/HTML)
* Resume rewriting
* Model selection UI
* Dashboard with charts
* Docker container

---

# â¤ï¸ **Contributing**

PRs are welcome.
Improve prompts, models, or add more career analytics.

---

# ğŸ“œ **License**

MIT License Â© 2025

---


#   S k i l l - G a p - A n a l y z e r - L o c a l - L L M - R A G - 
 
 
